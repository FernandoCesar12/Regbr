{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pegando os links dos anos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.select import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time\n",
    "\n",
    "chrome_options = Options()\n",
    "#chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    \n",
    "url = \"http://www4.planalto.gov.br/legislacao/portal-legis/legislacao-1/decretos1/decretos-1\"\n",
    "\n",
    "driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
    "driver.get(url)\n",
    "\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "href = ' '.join([str(elem) for elem in soup]).split('title=\"')\n",
    "\n",
    "links_ano = []\n",
    "for i in range(0,len(href)):\n",
    "    if 'class=\"internal-link\"' in str(href[i]):\n",
    "        result = str(href[i]).split('\"internal-link\" href=\"')[1].split('\" target=\"')[0]\n",
    "        links_ano.append(result)\n",
    "        \n",
    "links_ano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pegando os links dos PDF's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "#chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
    "\n",
    "link_pdf = []\n",
    "\n",
    "for url in links_ano:\n",
    "    \n",
    "    driver.get(url)\n",
    "\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "    href = ' '.join([str(elem) for elem in soup]).split('<tr class=\"visaoQuadrosTr\">')\n",
    "\n",
    "    link_pdf_ano = []\n",
    "\n",
    "    for i in range(0,len(href)):\n",
    "        if '<td class=\"visaoQuadrosTd\">\\n<a' in str(href[i]):\n",
    "            result = str(href[i]).split('href=\"')[1].split('\">\\n')[0]\n",
    "            link_pdf_ano.append(result)\n",
    "        \n",
    "    link_pdf.append(link_pdf_ano)\n",
    "    \n",
    "# Transformando numa única lista \n",
    "\n",
    "flat_list_pdf = [item for sublist in link_pdf for item in sublist]\n",
    "\n",
    "for i in range(0,len(flat_list_pdf)):\n",
    "    if 'style=\"margin: 0' in str(flat_list_pdf[i]):\n",
    "        flat_list_pdf[i] = str(flat_list_pdf[i]).split('\"')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pegando o conteudo textual e o link de redirecionamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "chrome_options = Options()\n",
    "#chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
    "\n",
    "Conteudo_textual = []\n",
    "Conteudo_link = []\n",
    "\n",
    "for url in flat_list_pdf:\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        driver.get(url)\n",
    "\n",
    "        driver.implicitly_wait(2)\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    \n",
    "        Conteudo = soup.find_all('p')\n",
    "    \n",
    "        texto = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", str(Conteudo)).replace('\\n','').replace(', \\xa0','')\n",
    "    \n",
    "        Link = str(soup.find_all('a')).split('href=\"')[1].split('\">')[0]\n",
    "        \n",
    "        if Conteudo_link !=  '':\n",
    "    \n",
    "            Conteudo_textual.append(texto)\n",
    "            Conteudo_link.append(Link)\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        print(f'Erro de leitura no link: {url}')\n",
    "\n",
    "for i in range(0,len(Conteudo_link)):\n",
    "    if '\" style=\"color:' in str(Conteudo_link[i]):\n",
    "        Conteudo_link[i] = str(Conteudo_link[i]).split('\" style=\"color:')[0]\n",
    "        \n",
    "for i in range(0,len(Conteudo_link)):\n",
    "    if 'Document&amp' in str(Conteudo_link[i]):\n",
    "        Conteudo_link[i] = str(Conteudo_link[i]).split('Document&amp')[0]+'Document'\n",
    "        \n",
    "Conteudo_link_2 = []\n",
    "for i in range(0,len(Conteudo_link)):\n",
    "    if '@' in str(Conteudo_link[i]):\n",
    "        Conteudo_link_2.append(Conteudo_link[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraindo informações adicionais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "#chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
    "\n",
    "Data_assinatura = []\n",
    "Ementa = []\n",
    "Situacao = []\n",
    "Chefe_governo = []\n",
    "Origem = []\n",
    "Data_publicacao = []\n",
    "Fonte = []\n",
    "Referenda = []\n",
    "Alteracao = []\n",
    "Veto = []\n",
    "Assunto = []\n",
    "Classificacao_direito = []\n",
    "Observacao = []\n",
    "Correlacao = []\n",
    "Titulo = []\n",
    "error_site = []\n",
    "\n",
    "for url in Conteudo_link_2:\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        driver.get(url)\n",
    "\n",
    "        driver.implicitly_wait(2)\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    \n",
    "        items_titulo = soup.find_all(\"h1\", {\"class\": \"text-center font-weight-bold title text-uppercase\"})\n",
    "        Titulo.append(str(items_titulo[0]).split('text-uppercase\">')[1].split('</h1>')[0])\n",
    "    \n",
    "        items = soup.find_all(\"li\", {\"class\": \"list-group-item border-0 p-0\"})\n",
    "        href = ' '.join([str(elem) for elem in items]).split('<div class=\"col-sm-2 label p-2\">')\n",
    "\n",
    "        #Separando as informações textuais\n",
    "    \n",
    "        for i in range(0,len(href)):\n",
    "            try:\n",
    "                if 'Data de assinatura:' in str(href[i]):\n",
    "                    result = str(href[i]).split('text-justify\">')[1].split('/div>')[0].replace('\\t','').replace('\\n','').replace('  ','').replace('<','').replace('br/>','')\n",
    "                    Data_assinatura.append(result)\n",
    "            except:\n",
    "                Data_assinatura.append('')\n",
    "            \n",
    "    \n",
    "        for i in range(0,len(href)):\n",
    "            try:\n",
    "                if 'Ementa:' in str(href[i]):\n",
    "                    result = str(href[i]).split('text-justify\">')[1].split('/div>')[0].replace('\\t','').replace('\\n','').replace('  ','').replace('<','').replace('br/>','')\n",
    "                    Ementa.append(result)\n",
    "            except:\n",
    "                Ementa.append('')\n",
    "        \n",
    "    \n",
    "        for i in range(0,len(href)):\n",
    "            try:\n",
    "                if 'Situação:' in str(href[i]):\n",
    "                    result = str(href[i]).split('text-justify\">')[1].split('/div>')[0].replace('\\t','').replace('\\n','').replace('  ','').replace('<','').replace('br/>','')\n",
    "                    Situacao.append(result)\n",
    "            except:\n",
    "                Situacao.append('')\n",
    "            \n",
    "    \n",
    "        for i in range(0,len(href)):\n",
    "            try:\n",
    "                if 'Chefe de Governo:' in str(href[i]):\n",
    "                    result = str(href[i]).split('text-justify\">')[1].split('/div>')[0].replace('\\t','').replace('\\n','').replace('  ','').replace('<','').replace('br/>','')\n",
    "                    Chefe_governo.append(result)\n",
    "            except:\n",
    "            \n",
    "                Chefe_governo.append('')\n",
    "        \n",
    "    \n",
    "        for i in range(0,len(href)):\n",
    "            try:\n",
    "                if 'Origem:' in str(href[i]):\n",
    "                    result = str(href[i]).split('text-justify\">')[1].split('/div>')[0].replace('\\t','').replace('\\n','').replace('  ','').replace('<','').replace('br/>','')\n",
    "                    Origem.append(result)\n",
    "            except:\n",
    "            \n",
    "                Origem.append('')\n",
    "        \n",
    "    \n",
    "        for i in range(0,len(href)):\n",
    "            try:\n",
    "                if 'Data de Publicação:' in str(href[i]):\n",
    "                    result = str(href[i]).split('text-justify\">')[1].split('/div>')[0].replace('\\t','').replace('\\n','').replace('  ','').replace('<','').replace('br/>','')\n",
    "                    Data_publicacao.append(result)\n",
    "            except:\n",
    "            \n",
    "                Data_publicacao.append('')\n",
    "            \n",
    "    \n",
    "        for i in range(0,len(href)):\n",
    "            try:\n",
    "                try:\n",
    "                    if 'Fonte:' in str(href[i]):\n",
    "                        result = str(href[i]).split('text-justify\">')[1].split('/div>')[0].replace('\\t','').replace('\\n','').replace('  ','').replace('<','').replace('br/>','').split('href=\"')[1].split('\" target=')[0].replace('amp;','')\n",
    "                        Fonte.append(result)\n",
    "            \n",
    "                except:\n",
    "                    result = str(href[i]).split('text-justify\">')[1].split('/div>')[0].replace('\\t','').replace('\\n','').replace('  ','').replace('<','').replace('br/>','').replace('amp;','')\n",
    "                    Fonte.append(result)\n",
    "            except:\n",
    "                Fonte.append('')\n",
    "        \n",
    "    \n",
    "        for i in range(0,len(href)):\n",
    "            try:\n",
    "                if 'Referenda:' in str(href[i]):\n",
    "                    result = str(href[i]).split('text-justify\">')[1].split('/div>')[0].replace('\\t','').replace('\\n','').replace('  ','').replace('<','').replace('br/>','')\n",
    "                    Referenda.append(result)\n",
    "            except:\n",
    "                Referenda.append('')\n",
    "        \n",
    "    \n",
    "        for i in range(0,len(href)):\n",
    "            try:\n",
    "                if 'Alteração:' in str(href[i]):\n",
    "                    result = str(href[i]).split('text-justify\">')[1].split('/div>')[0].replace('\\t','').replace('\\n','').replace('  ','').replace('<','').replace('br/>','')\n",
    "                    Alteracao.append(result)\n",
    "            except:\n",
    "                Alteracao.append('')\n",
    "        \n",
    "    \n",
    "        for i in range(0,len(href)):\n",
    "            try:\n",
    "                if 'Correlação:' in str(href[i]):\n",
    "                    result = str(href[i]).split('text-justify\">')[1].split('/div>')[0].replace('\\t','').replace('\\n','').replace('  ','').replace('<','').replace('br/>','')\n",
    "                    Correlacao.append(result)\n",
    "            except:\n",
    "                Correlacao.append('')\n",
    "        \n",
    "    \n",
    "        for i in range(0,len(href)):\n",
    "            try:\n",
    "                if 'Veto:' in str(href[i]):\n",
    "                    result = str(href[i]).split('text-justify\">')[1].split('/div>')[0].replace('\\t','').replace('\\n','').replace('  ','').replace('<','').replace('br/>','')\n",
    "                    Veto.append(result)\n",
    "            except:\n",
    "                Veto.append('')\n",
    "        \n",
    "    \n",
    "        for i in range(0,len(href)):\n",
    "            try:\n",
    "                if 'Assunto:' in str(href[i]):\n",
    "                    result = str(href[i]).split('text-justify\">')[1].split('/div>')[0].replace('\\t','').replace('\\n','').replace('  ','').replace('<','').replace('br/>','')\n",
    "                    Assunto.append(result)\n",
    "            except:\n",
    "                Assunto.append('')\n",
    "    \n",
    "        for i in range(0,len(href)):\n",
    "            try:\n",
    "                if 'Classificação de direito:' in str(href[i]):\n",
    "                    result = str(href[i]).split('text-justify\">')[1].split('/div>')[0].replace('\\t','').replace('\\n','').replace('  ','').replace('<','').replace('br/>','')\n",
    "                    Classificacao_direito.append(result)\n",
    "            except:\n",
    "                Classificacao_direito.append('')\n",
    "        \n",
    "    \n",
    "        for i in range(0,len(href)):\n",
    "            try:\n",
    "                if 'Observação:' in str(href[i]):\n",
    "                    result = str(href[i]).split('text-justify\">')[1].split('/div>')[0].replace('\\t','').replace('\\n','').replace('  ','').replace('<','').replace('br/>','')\n",
    "                    Observacao.append(result)\n",
    "            except:\n",
    "                Observacao.append('')\n",
    "                \n",
    "    except:\n",
    "        \n",
    "        error_site.append(url)\n",
    "        print(f'Erro na obtenção dos dados referentes ao link: {url}')\n",
    "        \n",
    "        Data_assinatura.append(' ')\n",
    "        Situacao.append(' ')\n",
    "        Chefe_governo.append(' ')\n",
    "        Origem.append(' ')\n",
    "        Data_publicacao.append(' ')\n",
    "        Fonte.append(' ')\n",
    "        Referenda.append(' ')\n",
    "        Correlacao.append(' ')\n",
    "        Alteracao.append(' ')\n",
    "        Veto.append(' ')\n",
    "        Assunto.append(' ')\n",
    "        Classificacao_direito.append(' ')\n",
    "        Observacao.append(' ')\n",
    "        Ementa.append(' ')\n",
    "        \n",
    "        if Titulo !=  '':\n",
    "            Titulo.append(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exportando em CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um DataFrame para alocar os outputs\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "BANCO = pd.DataFrame (Titulo ,columns=['Decreto'])\n",
    "BANCO['Link'] = Conteudo_link_2\n",
    "BANCO['Texto'] = Conteudo_textual\n",
    "BANCO['Data de assinatura'] = Data_assinatura\n",
    "BANCO['Ementa'] = Ementa\n",
    "BANCO['Situação'] = Situacao\n",
    "BANCO['Chefe de Governo'] = Chefe_governo\n",
    "BANCO['Origem'] = Origem\n",
    "BANCO['Data de publicação'] = Data_publicacao\n",
    "BANCO['Fonte'] = Fonte\n",
    "BANCO['Referenda'] = Referenda\n",
    "BANCO['Alteração'] = Alteracao\n",
    "BANCO['Veto'] = Veto\n",
    "BANCO['Classificação de direito'] = Classificacao_direito\n",
    "BANCO['Observação'] = Observacao\n",
    "BANCO['Correlação'] = Correlacao\n",
    "\n",
    "# Removendo páginas bloqueadas pelo governo \n",
    "\n",
    "BANCO = BANCO[BANCO['Texto'] != ']']\n",
    "\n",
    "excelfilename = 'Decretos'+ time.strftime(\"%d-%m-%Y\") +\".txt\"\n",
    "\n",
    "BANCO.to_csv(excelfilename, index=False, encoding='utf-8-sig', sep = '汉')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANCO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
